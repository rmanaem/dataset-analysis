{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "immune-status",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, ArrayType\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StopWordsRemover\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.tree import RandomForest\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import desc, size, max, abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "generic-cowboy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "vocal-packet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label the data sets, remove unncessary fields, sample and save the samples\n",
    "fake = pd.read_csv(\"../data/Fake.csv\")\n",
    "fake['label'] = 'Fake'\n",
    "fake = fake.drop(columns=[\"date\", \"subject\"])\n",
    "fake = fake.sample(21000)\n",
    "true = pd.read_csv(\"../data/True.csv\")\n",
    "true['label'] = 'True'\n",
    "true = true.drop(columns=[\"date\", \"subject\"])\n",
    "true = true.sample(20000)\n",
    "# fake.to_csv(\"../data/Sample2_Fake.csv\", index=False, header=None)\n",
    "# true.to_csv(\"../data/Sample2_True.csv\", index=False, header=None)\n",
    "\n",
    "# Extrac the stop words and punctuations and save them to a file\n",
    "punc = list(string.punctuation) + ['“','”','‘', '’','...','']\n",
    "stop_punc = stopwords.words('english') + punc\n",
    "# with open('../data/stop_punc.txt', 'w') as file:\n",
    "#     file.write(str(stop_punc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "structural-characteristic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification of  39855  datapoints containing\n",
      "19939  fake datapoints\n",
      "19916  true datapoints\n",
      "Using 5NN classifier and 5fold cross validation resulted in average f1 score of 0.9777375436010749\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing articles and removing stop words from the article\n",
    "fake_rdd = spark.read.csv(\"../data/Sample2_Fake.csv\").rdd\n",
    "fake_rdd = fake_rdd.filter(lambda x: x[0] is not None and x[1] is not None).map(lambda x: (x[0] + ' ' + x[1], x[2])).filter(lambda x: x[1]=='Fake')\n",
    "fake_rdd = fake_rdd.map(lambda x: Row(article=x[0], label=x[1]))\n",
    "fake_df = spark.createDataFrame(fake_rdd)\n",
    "tokenizer = Tokenizer(inputCol=\"article\", outputCol=\"words\")\n",
    "fake_df = tokenizer.transform(fake_df)\n",
    "fake_rdd = fake_df.rdd\n",
    "fake_rdd = fake_rdd.map(lambda x: (x[0], [i for i in x[2] if i not in stop_punc], x[1]))\n",
    "num_fake = fake_rdd.count()\n",
    "true_rdd = spark.read.csv(\"../data/Sample2_True.csv\").rdd\n",
    "true_rdd = true_rdd.filter(lambda x: x[0] is not None and x[1] is not None).map(lambda x: (x[0] + ' ' + x[1], x[2])).filter(lambda x: x[1]=='True')\n",
    "true_rdd = true_rdd.map(lambda x:Row(article=x[0], label=x[1]))\n",
    "true_df = spark.createDataFrame(true_rdd)\n",
    "true_df = tokenizer.transform(true_df)\n",
    "true_rdd = true_df.rdd.map(lambda x: (x[0], [i for i in x[2] if i not in stop_punc], x[1]))\n",
    "num_true = true_rdd.count()\n",
    "\n",
    "# Feature extraction using TFIDF\n",
    "fake_rdd = fake_rdd.map(lambda x: (x[1], x[2])).map(lambda x: Row(words=x[0], label=x[1]))\n",
    "fake_df = spark.createDataFrame(fake_rdd)\n",
    "hashingTF = HashingTF(inputCol='words', outputCol='rawFeatures', numFeatures=20)\n",
    "fake_df = hashingTF.transform(fake_df)\n",
    "idf = IDF(inputCol='rawFeatures', outputCol='features')\n",
    "idfModel = idf.fit(fake_df)\n",
    "fake_df = idfModel.transform(fake_df)\n",
    "fake_rdd = fake_df.rdd.map(lambda x: (x[3], x[1])).map(lambda x: ([np.take(x[0], i) for i in range(np.size(x[0]))], x[1]))\n",
    "\n",
    "true_rdd = true_rdd.map(lambda x: (x[1], x[2])).map(lambda x: Row(words=x[0], label=x[1]))\n",
    "true_df = spark.createDataFrame(true_rdd)\n",
    "hashingTF = HashingTF(inputCol='words', outputCol='rawFeatures', numFeatures=20)\n",
    "true_df = hashingTF.transform(true_df)\n",
    "idf = IDF(inputCol='rawFeatures', outputCol='features')\n",
    "idfModel = idf.fit(true_df)\n",
    "true_df = idfModel.transform(true_df)\n",
    "true_rdd = true_df.rdd.map(lambda x: (x[3], x[1])).map(lambda x: ([np.take(x[0], i) for i in range(np.size(x[0]))], x[1]))\n",
    "\n",
    "# Spliting the data set into training and test set using kfold cv\n",
    "kf = KFold(n_splits=5)\n",
    "fake = fake_rdd.collect()\n",
    "true = true_rdd.collect()\n",
    "fake_data = []\n",
    "for train_index, test_index in kf.split(fake):\n",
    "    train = []\n",
    "    test = []\n",
    "    for i in train_index:\n",
    "        train.append(fake[i])\n",
    "    for i in test_index:\n",
    "        test.append(fake[i])\n",
    "    fake_data.append((train, test))\n",
    "true_data = []\n",
    "for train_index, test_index in kf.split(true):\n",
    "    train = []\n",
    "    test = []\n",
    "    for i in train_index:\n",
    "        train.append(true[i])\n",
    "    for i in test_index:\n",
    "        test.append(true[i])\n",
    "    true_data.append((train, test))\n",
    "data = []\n",
    "for i in range(len(fake_data)):\n",
    "    true_data[i][0].extend(fake_data[i][0])\n",
    "    true_data[i][1].extend(fake_data[i][1])\n",
    "data = true_data\n",
    "\n",
    "# Training and testing using KNN classifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "accuracy = []\n",
    "predict = []\n",
    "f1 = []\n",
    "for i in range(len(data)):\n",
    "    knn.fit([j[0] for j in data[i][0]], [j[1] for j in data[i][0]])\n",
    "    accuracy.append(knn.score([j[0] for j in data[i][1]], [j[1] for j in data[i][1]]))\n",
    "    predict.append(knn.predict([j[0] for j in data[i][1]]))\n",
    "for i in range(len(predict)):\n",
    "    f1.append(f1_score([j[1] for j in data[i][1]], predict[i].tolist(), pos_label=\"True\"))\n",
    "average_f1 = sum(f1)/len(f1)\n",
    "\n",
    "# Result\n",
    "print('Classification of', num_fake + num_true, 'datapoints containing')\n",
    "print(num_fake, 'fake datapoints')\n",
    "print(num_true, 'true datapoints')\n",
    "print('Using 5NN classifier and 5fold cross validation resulted in average f1 score of', average_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "respected-flood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification of 39855 datapoints containing: \n",
      "19939 fake datapoints\n",
      "19916 true datapoints\n",
      "Using random forest classifier with 40 trees, max depth of 20 and 5fold cross validation resulted in average f1 score of 0.9999748964478474\n"
     ]
    }
   ],
   "source": [
    "# Tokenizing articles and removing stop words from the article\n",
    "fake_rdd = spark.read.csv(\"../data/Sample2_Fake.csv\").rdd\n",
    "fake_rdd = fake_rdd.filter(lambda x: x[0] is not None and x[1] is not None).map(lambda x: (x[0] + ' ' + x[1], x[2])).filter(lambda x: x[1]=='Fake')\n",
    "fake_rdd = fake_rdd.map(lambda x: Row(article=x[0], label=x[1]))\n",
    "fake_df = spark.createDataFrame(fake_rdd)\n",
    "tokenizer = Tokenizer(inputCol=\"article\", outputCol=\"words\")\n",
    "fake_df = tokenizer.transform(fake_df)\n",
    "fake_rdd = fake_df.rdd\n",
    "fake_rdd = fake_rdd.map(lambda x: (x[0], [i for i in x[2] if i not in stop_punc], x[1]))\n",
    "num_fake = fake_rdd.count()\n",
    "true_rdd = spark.read.csv(\"../data/Sample2_True.csv\").rdd\n",
    "true_rdd = true_rdd.filter(lambda x: x[0] is not None and x[1] is not None).map(lambda x: (x[0] + ' ' + x[1], x[2])).filter(lambda x: x[1]=='True')\n",
    "true_rdd = true_rdd.map(lambda x:Row(article=x[0], label=x[1]))\n",
    "true_df = spark.createDataFrame(true_rdd)\n",
    "true_df = tokenizer.transform(true_df)\n",
    "true_rdd = true_df.rdd.map(lambda x: (x[0], [i for i in x[2] if i not in stop_punc], x[1]))\n",
    "num_true = true_rdd.count()\n",
    "\n",
    "# Feature extraction using TFIDF\n",
    "fake_rdd = fake_rdd.map(lambda x: (x[1], x[2])).map(lambda x: Row(words=x[0], label=x[1]))\n",
    "fake_df = spark.createDataFrame(fake_rdd)\n",
    "hashingTF = HashingTF(inputCol='words', outputCol='rawFeatures', numFeatures=20)\n",
    "fake_df = hashingTF.transform(fake_df)\n",
    "idf = IDF(inputCol='rawFeatures', outputCol='features')\n",
    "idfModel = idf.fit(fake_df)\n",
    "fake_df = idfModel.transform(fake_df)\n",
    "fake_rdd = fake_df.rdd.map(lambda x: (x[3], x[1])).map(lambda x: ([np.take(x[0], i) for i in range(np.size(x[0]))], x[1]))\n",
    "\n",
    "true_rdd = true_rdd.map(lambda x: (x[1], x[2])).map(lambda x: Row(words=x[0], label=x[1]))\n",
    "true_df = spark.createDataFrame(true_rdd)\n",
    "hashingTF = HashingTF(inputCol='words', outputCol='rawFeatures', numFeatures=20)\n",
    "true_df = hashingTF.transform(true_df)\n",
    "idf = IDF(inputCol='rawFeatures', outputCol='features')\n",
    "idfModel = idf.fit(true_df)\n",
    "true_df = idfModel.transform(true_df)\n",
    "true_rdd = true_df.rdd.map(lambda x: (x[3], x[1])).map(lambda x: ([np.take(x[0], i) for i in range(np.size(x[0]))], x[1]))\n",
    "\n",
    "# Spliting the data set into training and test set using kfold cv\n",
    "kf = KFold(n_splits=5)\n",
    "fake = fake_rdd.collect()\n",
    "true = true_rdd.collect()\n",
    "fake_data = []\n",
    "for train_index, test_index in kf.split(fake):\n",
    "    train = []\n",
    "    test = []\n",
    "    for i in train_index:\n",
    "        train.append(fake[i])\n",
    "    for i in test_index:\n",
    "        test.append(fake[i])\n",
    "    fake_data.append((train, test))\n",
    "true_data = []\n",
    "for train_index, test_index in kf.split(true):\n",
    "    train = []\n",
    "    test = []\n",
    "    for i in train_index:\n",
    "        train.append(true[i])\n",
    "    for i in test_index:\n",
    "        test.append(true[i])\n",
    "    true_data.append((train, test))\n",
    "data = []\n",
    "for i in range(len(fake_data)):\n",
    "    true_data[i][0].extend(fake_data[i][0])\n",
    "    true_data[i][1].extend(fake_data[i][1])\n",
    "data = true_data\n",
    "\n",
    "# Training and testing using random forest classifier \n",
    "num_trees = 40\n",
    "max_depth = 20\n",
    "rf = RandomForestClassifier(n_estimators=num_trees, max_depth=max_depth)\n",
    "accuracy = []\n",
    "predict = []\n",
    "f1 = []\n",
    "for i in range(len(data)):\n",
    "    rf.fit([j[0] for j in data[i][0]], [j[1] for j in data[i][0]])\n",
    "    accuracy.append(rf.score([j[0] for j in data[i][1]], [j[1] for j in data[i][1]]))\n",
    "    predict.append(rf.predict([j[0] for j in data[i][1]]))\n",
    "for i in range(len(predict)):\n",
    "    f1.append(f1_score([j[1] for j in data[i][1]], predict[i].tolist(), pos_label=\"True\"))\n",
    "average_f1 = sum(f1)/len(f1)\n",
    "\n",
    "# Result\n",
    "print('Classification of', num_fake + num_true, 'datapoints containing: ')\n",
    "print(num_fake, 'fake datapoints')\n",
    "print(num_true, 'true datapoints')\n",
    "print('Using random forest classifier with', num_trees, 'trees, max depth of', max_depth, 'and 5fold cross validation resulted in average f1 score of', average_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-somewhere",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
