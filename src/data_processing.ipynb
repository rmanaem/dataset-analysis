{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "immune-status",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import sys\n",
    "import random\n",
    "import string\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "from sklearn.utils import shuffle\n",
    "from nltk.corpus import stopwords  \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.rdd import RDD\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import desc, size, max, abs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "generic-cowboy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "vocal-packet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label the data sets and remove unncessary fields\n",
    "fake = pd.read_csv(\"../data/Fake.csv\")\n",
    "true = pd.read_csv(\"../data/True.csv\")\n",
    "fake = fake.assign(label=\"Fake\")\n",
    "fake = fake.drop(columns=[\"date\", \"subject\"])\n",
    "true = true.assign(label=\"True\")\n",
    "true = true.drop(columns=[\"date\", \"subject\"])\n",
    "fake = fake.head(2150)\n",
    "true = true.head(2150)\n",
    "fake.to_csv(\"../data/Sample_Fake.csv\", index=False, header=None)\n",
    "true.to_csv(\"../data/Sample_True.csv\", index=False, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "collect-conversion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stop words from the article context \n",
    "punc = list(string.punctuation) + ['“','”','‘', '’','...']\n",
    "stop_words = stopwords.words('english') + punc\n",
    "true_rdd = spark.read.csv(\"../data/Sample_True.csv\").rdd\n",
    "true_rdd = true_rdd.map(lambda x: [x[0], x[1], x[2]])\n",
    "true_rdd = true_rdd.map(lambda x: [x[0], [i for i in word_tokenize(x[1].lower()) if i not in stop_words], x[2]])\n",
    "fake_rdd = spark.read.csv(\"../data/Sample_Fake.csv\").rdd\n",
    "fake_rdd = fake_rdd.map(lambda x: [x[0], x[1], x[2]]).filter(lambda x: x[1] is not None)\n",
    "fake_rdd = fake_rdd.map(lambda x: [x[0], [i for i in word_tokenize(x[1].lower()) if i not in stop_words], x[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "structural-characteristic",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
